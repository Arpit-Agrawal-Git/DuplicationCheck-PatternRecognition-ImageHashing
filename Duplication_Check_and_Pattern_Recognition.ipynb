{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485547ad",
   "metadata": {},
   "source": [
    "### Duplication Check | Pattern Recognition | Image Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d260c",
   "metadata": {},
   "source": [
    "#### We start with extraction of images from emails of new claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b847e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 561/561 [02:21<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# IMAGES FROM PDF IMAGES\n",
    "import os\n",
    "import extract_msg\n",
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory containing the saved emails\n",
    "email_directory = r\"C:\\PRA Image Project\\Emails\"\n",
    "# email_directory = r\"C:\\Users\\arpitag\\Downloads\\Interio\\Smartnet PRA mails\"\n",
    "# Directory to save the images\n",
    "image_directory = r\"C:\\PRA Image Project\\New\"\n",
    "# image_directory = r\"C:\\Users\\arpitag\\Downloads\\Interio\\Pics\"\n",
    "\n",
    "if not os.path.exists(image_directory):\n",
    "    os.makedirs(image_directory)\n",
    "\n",
    "poppler_path = r\"C:\\Users\\arpitag\\AppData\\Local\\Programs\\poppler-23.11.0\\Library\\bin\" \n",
    "\n",
    "def is_image_file(filename):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']\n",
    "    return any(filename.lower().endswith(ext) for ext in image_extensions)\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, output_folder, base_filename):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        if doc.is_encrypted:\n",
    "            try:\n",
    "                doc.authenticate('')  # Attempt to decrypt with an empty password\n",
    "            except Exception as e:\n",
    "                return\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            for img_index, img in enumerate(doc.get_page_images(page_num)):\n",
    "                xref = img[0]\n",
    "                pix = fitz.Pixmap(doc, xref)\n",
    "                if pix.n - pix.alpha < 4:  # this is GRAY or RGB\n",
    "                    pix.save(os.path.join(output_folder, f\"{base_filename}_page{page_num + 1}_img{img_index}.png\"))\n",
    "                else:  # CMYK: convert to RGB first\n",
    "                    fitz.Pixmap(fitz.csRGB, pix).save(os.path.join(output_folder, f\"{base_filename}_page{page_num + 1}_img{img_index}.png\"))\n",
    "                pix = None\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "# Process each email file\n",
    "for filename in tqdm(os.listdir(email_directory)):\n",
    "    if filename.endswith('.msg'):\n",
    "        file_path = os.path.join(email_directory, filename)\n",
    "        with extract_msg.Message(file_path) as msg:\n",
    "            for attachment in msg.attachments:\n",
    "                if is_image_file(attachment.longFilename or attachment.shortFilename):\n",
    "                    image_name = attachment.longFilename or attachment.shortFilename\n",
    "                    if image_name:\n",
    "                        file_path = os.path.join(image_directory, image_name)\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            f.write(attachment.data)\n",
    "                elif attachment.longFilename and attachment.longFilename.lower().endswith('.pdf'):\n",
    "                    # Temporarily save the PDF file\n",
    "                    temp_pdf_path = os.path.join(image_directory, attachment.longFilename)\n",
    "                    with open(temp_pdf_path, 'wb') as f:\n",
    "                        f.write(attachment.data)\n",
    "                    # Extract images from the PDF\n",
    "                    extract_images_from_pdf(temp_pdf_path, image_directory, os.path.splitext(attachment.longFilename)[0])\n",
    "                    #os.remove(temp_pdf_path)  # Clean up the temporary PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771b4b3",
   "metadata": {},
   "source": [
    "#### Now we check hash of each new image with hashes of all images of claims made in past one year. We compted the hashes of these past images separately and stored them in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba154913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import imagehash\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File to store hashes\n",
    "hashes_file = r\"C:\\PRA Image Project\\Hashes\\hash\\hashes.txt\"\n",
    "pdhashes_file = r\"C:\\PRA Image Project\\Hashes\\pdhash\\pdhashes.txt\"\n",
    "\n",
    "def save_hash_to_file(hash_file, img_hash, filename):\n",
    "    with open(hash_file, 'a') as file:\n",
    "        file.write(f\"{img_hash},{filename}\\n\")\n",
    "\n",
    "def load_hashes_from_file(hash_file):\n",
    "    loaded_hashes = {}\n",
    "    if os.path.exists(hash_file):\n",
    "        with open(hash_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) == 2:\n",
    "                    loaded_hashes[imagehash.hex_to_hash(parts[0])] = parts[1]\n",
    "    return loaded_hashes\n",
    "\n",
    "# Load previously computed hashes\n",
    "hashes = load_hashes_from_file(hashes_file)\n",
    "pdhashes = load_hashes_from_file(pdhashes_file)\n",
    "\n",
    "# Create sets of already processed filenames\n",
    "processed_filenames = set(hashes.values())\n",
    "processed_pd_filenames = set(pdhashes.values())\n",
    "\n",
    "# We use spreadd sheet style naming to name duplicate images so that they appear next to each other in the output folder\n",
    "def generate_labels(index):\n",
    "    # ASCII value for 'A'\n",
    "    ascii_start = 65\n",
    "    # Determine the number of letters in the label (A, B, ..., Z, AA, AB, ...)\n",
    "    num_letters = (index // 52) + 1\n",
    "    # Calculate label (A, B, C, ..., AA, AB, ...)\n",
    "    label = ''\n",
    "    for i in range(num_letters):\n",
    "        label += chr(ascii_start + ((index // 2) % 26))\n",
    "    # Calculate number (1 or 2)\n",
    "    number = 1 if index % 2 == 0 else 2\n",
    "    return f'{label}{number}'\n",
    "\n",
    "# Directories\n",
    "new_image_directory =  r\"C:\\PRA Image Project\\New\"\n",
    "# new_image_directory = \"C:\\PRA Image Project\\Z2_Meta Data Check\"\n",
    "# processed_image_directory =  r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\23-24\\PRA Sticker Match\\pics from pdfs\"\n",
    "processed_image_directory =  r\"C:\\PRA Image Project\\Processed\"\n",
    "duplicates_directory =  r\"C:\\PRA Image Project\\Duplicate\"\n",
    "# duplicates_directory =  r\"C:\\Users\\arpitag\\OneDrive - Godrej & Boyce Mfg. Co. Ltd\\Desktop\\Work\\23-24\\PRA Sticker Match\\Duplicates\"\n",
    "processed_duplicates_directory =  r\"C:\\PRA Image Project\\Processed Duplicates\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(duplicates_directory, exist_ok=True)\n",
    "os.makedirs(processed_image_directory, exist_ok=True)\n",
    "\n",
    "# Function to get image hash\n",
    "def get_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            return imagehash.average_hash(img)\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        print(f\"Cannot identify image file '{image_path}'. Skipping...\")\n",
    "        return None\n",
    "\n",
    "similarity_threshold = 0\n",
    "index = 0\n",
    "\n",
    "\n",
    "# Now check each image in the New folder\n",
    "for filename in tqdm(os.listdir(new_image_directory)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        file_path = os.path.join(new_image_directory, filename)\n",
    "        img_hash = get_image_hash(file_path)\n",
    "        \n",
    "        if img_hash is None:\n",
    "            continue\n",
    "        duplicate_found = False\n",
    "\n",
    "        for stored_hash, stored_filename in hashes.items():\n",
    "            if img_hash - stored_hash <= similarity_threshold:\n",
    "                if filename[:10]==stored_filename[:10]:\n",
    "                    duplicate_found = True\n",
    "                    break  # Skip processing this file as it's a duplicate with the same name prefix\n",
    "                \n",
    "                label1 = generate_labels(index)\n",
    "                label2 = generate_labels(index + 1)\n",
    "\n",
    "                original_path = os.path.join(processed_image_directory, stored_filename)\n",
    "                shutil.copy(original_path, os.path.join(duplicates_directory, f'{label1}_{stored_filename}'))\n",
    "                shutil.copy(file_path, os.path.join(duplicates_directory, f'{label2}_{filename}'))\n",
    "\n",
    "                index += 2\n",
    "                duplicate_found = True\n",
    "                break\n",
    "        #Check in Processed Duplicates folder        \n",
    "        for stored_hash, stored_filename in pdhashes.items():\n",
    "            if img_hash - stored_hash <= similarity_threshold:\n",
    "                if filename[:10]==stored_filename[:10]:\n",
    "                    duplicate_found = True\n",
    "                    break  # Skip processing this file as it's a duplicate with the same name prefix\n",
    "                \n",
    "                label1 = generate_labels(index)\n",
    "                label2 = generate_labels(index + 1)\n",
    "\n",
    "                original_path = os.path.join(processed_duplicates_directory, stored_filename)\n",
    "                shutil.copy(original_path, os.path.join(duplicates_directory, f'{label1}_{stored_filename}'))\n",
    "                shutil.copy(file_path, os.path.join(duplicates_directory, f'{label2}_{filename}'))\n",
    "\n",
    "                index += 2\n",
    "                duplicate_found = True\n",
    "                break\n",
    "\n",
    "        # Move the image from New to Processed (regardless of whether it's a duplicate)\n",
    "        shutil.move(file_path, os.path.join(processed_image_directory, filename))\n",
    "        if not duplicate_found:\n",
    "            # Add the hash of this new, unique image to the dictionary\n",
    "            hashes[img_hash] = filename\n",
    "            save_hash_to_file(hashes_file, img_hash, filename)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654749d3",
   "metadata": {},
   "source": [
    "#### Next we manually delete the FALSE POSITIVES and take further action on duplicates\n",
    "\n",
    "Few examples of duplicate images are as below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f569679",
   "metadata": {},
   "source": [
    "![alt text](<WhatsApp Image 2024-04-28 at 20.28.39_f7046697.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c3340",
   "metadata": {},
   "source": [
    "![alt text](<WhatsApp Image 2024-04-28 at 20.30.00_c4a0c533.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714dcfd",
   "metadata": {},
   "source": [
    "![alt text](<WhatsApp Image 2024-04-28 at 20.29.26_c4967af9.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d57a1",
   "metadata": {},
   "source": [
    "![alt text](<WhatsApp Image 2024-04-28 at 20.31.43_883f3369.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde019f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2133353",
   "metadata": {},
   "source": [
    "#### We also use the algorithm for Pattern Recognition\n",
    "\n",
    "Suspected Pattern Image Template is put in \"PATTERN TEMPLATE' foolder. All processed images are in 'Processed' folder.\n",
    "Results are copied to 'Patterns' folder.\n",
    "We use multiple values of threshold between 2 to 10 to find patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6276ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import imagehash\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File to store hashes\n",
    "hashes_file = r\"C:\\PRA Image Project\\Hashes\\hash\\hashes.txt\"\n",
    "pdhashes_file = r\"C:\\PRA Image Project\\Hashes\\pdhash\\pdhashes.txt\"\n",
    "\n",
    "def generate_labels(index):\n",
    "    # ASCII value for 'A'\n",
    "    ascii_start = 65\n",
    "    # Determine the number of letters in the label (A, B, ..., Z, AA, AB, ...)\n",
    "    num_letters = (index // 52) + 1\n",
    "    # Calculate label (A, B, C, ..., AA, AB, ...)\n",
    "    label = ''\n",
    "    for i in range(num_letters):\n",
    "        label += chr(ascii_start + ((index // 2) % 26))\n",
    "    # Calculate number (1 or 2)\n",
    "    number = 1 if index % 2 == 0 else 2\n",
    "    return f'{label}{number}'\n",
    "\n",
    "def save_hash_to_file(hash_file, img_hash, filename):\n",
    "    with open(hash_file, 'a') as file:\n",
    "        file.write(f\"{img_hash},{filename}\\n\")\n",
    "\n",
    "def load_hashes_from_file(hash_file):\n",
    "    loaded_hashes = {}\n",
    "    if os.path.exists(hash_file):\n",
    "        with open(hash_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) == 2:\n",
    "                    loaded_hashes[imagehash.hex_to_hash(parts[0])] = parts[1]\n",
    "    return loaded_hashes\n",
    "\n",
    "# Directories\n",
    "new_image_directory = r\"C:\\PRA Image Project\\PATTERN TEMPLATE\"\n",
    "processed_image_directory = r\"C:\\PRA Image Project\\Processed\"\n",
    "duplicates_directory = r\"C:\\PRA Image Project\\Patterns\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(duplicates_directory, exist_ok=True)\n",
    "os.makedirs(processed_image_directory, exist_ok=True)\n",
    "\n",
    "def get_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            return imagehash.average_hash(img)\n",
    "    except UnidentifiedImageError:\n",
    "        print(f'Warning: Could not identify image file {image_path}. Skipping...')\n",
    "        return None\n",
    "\n",
    "similarity_threshold = 10\n",
    "hashes = load_hashes_from_file(hashes_file)\n",
    "index = 0\n",
    "\n",
    "\n",
    "# Now check each image in the New folder\n",
    "for filename in tqdm(os.listdir(new_image_directory), desc=\"Checking new images\"):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        file_path = os.path.join(new_image_directory, filename)\n",
    "        img_hash = get_image_hash(file_path)\n",
    "        if img_hash is None:\n",
    "            continue  # Skip this file since its hash could not be obtained\n",
    "        matches = []\n",
    "\n",
    "        for stored_hash, stored_filename in tqdm(hashes.items()):\n",
    "            if img_hash - stored_hash <= similarity_threshold:\n",
    "                matches.append((stored_filename, file_path))\n",
    "\n",
    "        for match in tqdm(matches):\n",
    "            stored_filename, file_path = match\n",
    "            label = generate_labels(index)\n",
    "\n",
    "            original_path = os.path.join(processed_image_directory, stored_filename)\n",
    "            shutil.copy(original_path, os.path.join(duplicates_directory, f'{label}_{stored_filename}'))\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f9f2e",
   "metadata": {},
   "source": [
    "Few Examples of Pattern found in damage claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7c434",
   "metadata": {},
   "source": [
    "![alt text](<WhatsApp Image 2024-04-28 at 20.37.10_31093bad-1.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b1aa4",
   "metadata": {},
   "source": [
    "![alt text](<WhatsApp Image 2024-04-28 at 20.43.09_ba7bc865-1.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cea78",
   "metadata": {},
   "source": [
    "These insights are being regularly shared with Plant Quality and improvements in packaging, processes and design has been initiated to counter such issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbed99e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a5f2b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca9a969",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
